{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Давайте создадим AI чат бота..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Давайте обсудим, для чего это нужно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Сильный AI может стать бесполезным, если не сможет общаться с людьми\n",
    "2. Даже если не мечтать про сильный AI, умение \"понимать\" и осмысленно отвечать на человеческом языке может сильно помочь в автоматизации службы поддержки компании и др.\n",
    "3. Изучение новой технологии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Цель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Создание чат бота, поддерживающего беседу\n",
    "* Изучение технологий, необходимых для этого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Загрузим и подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /root/miniconda/envs/rep_py2/lib/libcrypto.so.1.0.0: no version information available (required by wget)\n",
      "wget: /root/miniconda/envs/rep_py2/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "--2017-02-20 09:50:06--  https://www.dropbox.com/s/tfeozvw4hfnmufu/subtitles.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/Eg62YRcosINCgVsOlmZ0JyEy6yHtfEq0ccS9OBBhTCxKcF2lvm8aEZd6RHY8qMuu/file [following]\n",
      "--2017-02-20 09:50:07--  https://dl.dropboxusercontent.com/content_link/Eg62YRcosINCgVsOlmZ0JyEy6yHtfEq0ccS9OBBhTCxKcF2lvm8aEZd6RHY8qMuu/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.66.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.66.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4407085 (4.2M) [text/plain]\n",
      "Saving to: 'subtitles.txt'\n",
      "\n",
      "100%[======================================>] 4,407,085   7.44MB/s   in 0.6s   \n",
      "\n",
      "2017-02-20 09:50:09 (7.44 MB/s) - 'subtitles.txt' saved [4407085/4407085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/tfeozvw4hfnmufu/subtitles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/cpcfs5y6z4qqhc0/Networks_weights.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/u9873fsq6r4he28/token_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "PAD_ix = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(\"subtitles.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    phrase_list = text.split(\"\\n\")\n",
    "    del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Гас Делэрио - в некотором смысле человек- невидимка.',\n",
       " 'думаешь, что знаешь о нём абсолютно всё,..... но на самом деле ты не знаешь ничего.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы генирируем ответы посимвольно, определимся, какие символы у нас встречаются "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_singhs = 'АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя0123456789.?!,;:-( )\"\"+=-_*' + \"''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = set(all_singhs)\n",
    "\n",
    "tokens = list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обозначения начала и конца будем использовать START и END. Превратим каждое наше предложение в список символов, которые оно содержит, добавим обозначение начала и конца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [\"START\"] + tokens + [\"END\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase_list = list(map(lambda x: [\"START\"] + list(x) + [\"END\"], phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94471"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь token - id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_to_id = {tok: i for i, tok in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {token_to_id[tok]: tok for tok in token_to_id.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Будем превращать все наши предложения в матрицу, имеющую форму (кол-во предложений, мак-ое кол-во символов в предложении), если остаются пустые ячейки заполним их -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences,token_to_i, max_len=None,PAX_ix=PAD_ix):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int16') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = list(filter(None.__ne__, map(token_to_i.get,seq)))[:max_len]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем в токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 68 76 37 82 68 84  8 43 33  1 22 70 68 46 68 80 68 62  8 27 70 31 70\n",
      "   1 70 73 68 82 73 26 82 43  8 68 69  8 43 70 80  8 27 46 68 62  8 80 22\n",
      "  29 22 73 27 37 78 93 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 29 67 73 37  8 91 32 42 68 69 31 70 68 89 62 37  8 91 32 68 70 68 62\n",
      "  36 73 68 37 92 82 70 43 52 31 62 70 68 80 82 36 42 78 78 78 78 78 68 62\n",
      "  70 68 62 37 68 82 37 73 70 73 68 29  8 43  8 68 31 26 68 62  8 68 89 62\n",
      "  37  8 91 32 68 62 22 69  8 48 70 78 93 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 84 37 77  8 68 81 70 82 73 70 31  1  8 80 68 62 37 68 82 81 22 82 70\n",
      "  27 68 37 27 31 36  1 70 80 42 68 27 70 31 70  1 26  8 68 82 62 22 73 37\n",
      "  43 22 82 32 68 80 68  8 48 70 68 28 22 43 32 73 37 79 42 78 78 78 78 78\n",
      "  68 67 77  8 68 82 31 37 62 70 80 22 31 82 85 68 85 82 62 70 42 68 69 31\n",
      "  70 68 70 62 68 22 73  8 43 68 67 82 81  8 79 78 93]\n",
      " [ 0 12 62 70 48 22  8 68 73  8 69 31 37 43 22 68 82 62 85 31 32 82 85 68\n",
      "  67 68 76 37 82 37 78 93 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 16 33 43 80 22 62 68 16 37 73 73 22 62 48 82 78 93 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "print(as_matrix(phrase_list[:5], token_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tokens_id.json\", \"w\") as fp:\n",
    "    json.dump({\"token_to_id\":token_to_id, \"id_to_token\": id_to_token, \"tokens\": tokens}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tokens_id.json\") as f:\n",
    "    tokens_id = json.load(f)\n",
    "token_to_id = tokens_id[u'token_to_id']\n",
    "id_to_token = tokens_id[\"id_to_token\"]\n",
    "tokens = tokens_id[\"tokens\"]\n",
    "tokens = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сеть, которая будет получать на вход вопрос и генерировать посимвольно ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from lasagne.updates import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence','int32')\n",
    "target_phonemes = T.matrix('target phonemes','int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1))\n",
    "l_emb = lasagne.layers.EmbeddingLayer(l_in, len(tokens), 40)\n",
    "l_rnn = lasagne.layers.LSTMLayer(l_emb,256,only_return_final=True,mask_input=l_mask)\n",
    "\n",
    "##DECODER\n",
    "transc_in = lasagne.layers.InputLayer(shape=(None, None),input_var=target_phonemes)\n",
    "transc_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=T.neq(target_phonemes,-1))\n",
    "transc_emb = lasagne.layers.EmbeddingLayer(transc_in, len(tokens), 50)\n",
    "transc_rnn = lasagne.layers.LSTMLayer(transc_emb,256,hid_init=l_rnn,mask_input=transc_mask)\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "transc_rnn_flat = lasagne.layers.reshape(transc_rnn, (-1,transc_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(transc_rnn_flat,len(tokens),nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, b]\n"
     ]
    }
   ],
   "source": [
    "weights = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "network_output = network_output.reshape([target_phonemes.shape[0], target_phonemes.shape[1], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_flat = network_output[:,:-1,:].reshape([-1, len(tokens)])\n",
    "targets = target_phonemes[:,1:].ravel()\n",
    "\n",
    "mask = T.nonzero(T.neq(targets, -1))\n",
    "\n",
    "loss = categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "updates = adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компилируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING:theano.configdefaults:install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "WARNING:theano.tensor.blas:We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#training\n",
    "train = theano.function([input_sequence, target_phonemes], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_phonemes], loss, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим генератор ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "#reshape back into original shape\n",
    "network_output = network_output.reshape((target_phonemes.shape[0],target_phonemes.shape[1],len(tokens)))\n",
    "#predictions for next tokens (after sequence end)\n",
    "last_word_probas = network_output[:,-1]\n",
    "probs = theano.function([input_sequence,target_phonemes],last_word_probas,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(question,answer_prefix = (\"START\",),t=1,sample=True, max_len=500):\n",
    "    \n",
    "    answer = list(answer_prefix)\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        next_let_probs = probs(as_matrix([question],token_to_id),as_matrix([answer],token_to_id) ).ravel()\n",
    "        next_let_probs = next_let_probs**t / np.sum(next_let_probs**t)\n",
    "\n",
    "        if sample:\n",
    "            next_letter = np.random.choice(tokens,p=next_let_probs) \n",
    "        else:\n",
    "            next_letter = tokens[np.argmax(next_let_probs)]\n",
    "        \n",
    "        answer.append(next_letter)\n",
    "\n",
    "        if next_letter==\"END\":\n",
    "            break\n",
    "    return \"\".join(answer[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(\"Who are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим предобученые веса, если есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load(\"Networks_weights.npz\", encoding=\"bytes\") as weights_file:\n",
    "    lasagne.layers.set_all_param_values(l_out, weights_file[\"arr_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(source, batchsize, shuffle=True):\n",
    "    source = np.array(source)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(source)-1)\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(source) -1 - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = np.array(slice(start_idx, start_idx + batchsize))\n",
    "        yield as_matrix(source[excerpt], token_to_id), as_matrix(source[excerpt+1],token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "telegram_token = config.telegram_token\n",
    "telegram_chat_id = config.telegram_chat_id# id чата, в который будут отсылаться результаты в процессе обучения, так как она учится долго\n",
    "dropbox_token = config.dropbox_token# Свежие веса загружаются на в облако, чтобы не потерять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from telebot import TeleBot\n",
    "bot = TeleBot(telegram_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/jupyterhub_py3/lib/python3.4/site-packages/ipykernel/__main__.py:3: DeprecationWarning: You are using a deprecated client. Please use the new v2 client located at dropbox.Dropbox.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import dropbox\n",
    "\n",
    "client = dropbox.client.DropboxClient(dropbox_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_message = \"Epoch {epoch}:\\nTime {time_:.2f} hours\\nAverage loss: {avg_los:.5f}\\nExample sample=True: {sample_0}\"\n",
    "train_error_messages = \"Error on epoch when we train model: {}\\n\\t{}\\nBatch:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "num_epoch = 100\n",
    "batch_per_epoch = 1000\n",
    "batch_size = 1000\n",
    "bot.send_message(145718567, \"Learning is begin!\")\n",
    "for epoch in range(num_epoch):\n",
    "    try:\n",
    "        \n",
    "        st = time()\n",
    "        avg_cost = 0\n",
    "        for batch_num, batch in enumerate(iterate_minibatches(phrase_list, batch_size)):\n",
    "            try:\n",
    "                avg_cost += train(batch[0], batch[1])\n",
    "            except Exception as er:\n",
    "                bot.send_message(telegram_chat_id, train_error_messages.format(epoch+1, er, batch))\n",
    "                break\n",
    "                \n",
    "            if batch_num+1 == batch_per_epoch:\n",
    "                break\n",
    "                \n",
    "        weights_file = \"Networks_weights.npz\"\n",
    "        np.savez(weights_file, layers.get_all_param_values(l_out))\n",
    "        try:\n",
    "            with open(weights_file, 'rb') as weights_file_dr:\n",
    "                response = client.put_file('/' + weights_file, weights_file_dr)\n",
    "\n",
    "        except Exception as er:\n",
    "            bot.send_message(telegram_chat_id, \"Error of download to Dropbox: {}\".format(e))\n",
    "\n",
    "        bot.send_message(145718567, train_message.format(time_=(time()-st)/3600,\n",
    "                                                         epoch=epoch+1,\n",
    "                                                         avg_los=avg_cost/batch_per_epoch,\n",
    "                                                         sample_0=generate_answer(\"Who are you?\")))\n",
    "    \n",
    "    except Exception as er:\n",
    "        bot.send_message(telegram_chat_id, \"Error on epoch: {}\\n\\t{}\".format(epoch+1, er))\n",
    "        break\n",
    "bot.send_message(telegram_chat_id, \"Learning is end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought that you were the only one that were the one who was a sure.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(re.sub(\"\\n\", \" \",\"\"\"How are you?\"\"\"), t=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
