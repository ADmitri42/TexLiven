{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Давайте создадим AI чат бота..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Давайте обсудим, для чего это нужно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Сильный AI может стать бесполезным, если не сможет общаться с людьми\n",
    "2. Даже если не мечтать про сильный AI, умение \"понимать\" и осмысленно отвечать на человеческом языке может сильно помочь в автоматизации службы поддержки компании и др.\n",
    "3. Изучение новой технологии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Цель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Создание чат бота, поддерживающего беседу\n",
    "* Изучение технологий, необходимых для этого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Загрузим и подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/tfeozvw4hfnmufu/subtitles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/cpcfs5y6z4qqhc0/Networks_weights.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/u9873fsq6r4he28/token_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "PAD_ix = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(text):\n",
    "    rep = re.findall(r\">>>[А-я ]+[\\.] [A-я ,;:-]+[\\.\\?\\!\\]]\", text)\n",
    "    return list(map(lambda x: re.sub(r\">>>[А-я ]+[\\.] \", \"\", x), rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = []\n",
    "for filename in os.listdir(\"data\"):\n",
    "    with codecs.open(\"data/{}\".format(filename), 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()\n",
    "        phrase_list.extend(split(text))\n",
    "        del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Вот тут и живи, как хочешь.',\n",
       " 'У вас ведь, маменька, уж один разговор.',\n",
       " 'Что ж такое не говорить-то!',\n",
       " 'Разве я виновата, маменька, что мне никто не нравится?',\n",
       " 'Какой же каприз, маменька?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symb = \"\"\n",
    "for s in phrase_list:\n",
    "    symb += s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы генирируем ответы посимвольно, определимся, какие символы у нас встречаются "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_singhs = 'АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя0123456789.?!,;:-( )\"\"-_*' + \"''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = set(symb)\n",
    "\n",
    "tokens = list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обозначения начала и конца будем использовать START и END. Превратим каждое наше предложение в список символов, которые оно содержит, добавим обозначение начала и конца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [\"START\"] + tokens + [\"END\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase_list = list(map(lambda x: [\"START\"] + list(x) + [\"END\"], phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4867"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь token - id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {tok: i for i, tok in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {token_to_id[tok]: tok for tok in token_to_id.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем превращать все наши предложения в матрицу, имеющую форму (кол-во предложений, мак-ое кол-во символов в предложении), если остаются пустые ячейки заполним их -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences,token_to_i, max_len=None,PAX_ix=PAD_ix):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = list(filter(None.__ne__, map(token_to_i.get,seq)))[:max_len]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем в токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 50 31  4 20  4 23  4 20 24 20 48 24 50 24 32 20  6 17  6 20  3 31 44\n",
      "  15 46 52 37 54 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 23 20 50 17  9 20 50 15 34 52 32 20 45 17 45 15 13 52  6 17 32 20 23\n",
      "  48 20 31 34 24 13 20 27 17 21 41 31 50 31 27 37 54 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 44  4 31 20 48 20  4 17  6 31 15 20 13 15 20 41 31 50 31 27 24  4 52\n",
      "  42  4 31 10 54 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 0 27 17 21 50 15 20 14 20 50 24 13 31 50 17  4 17 32 20 45 17 45 15 13\n",
      "  52  6 17 32 20 44  4 31 20 45 13 15 20 13 24  6  4 31 20 13 15 20 13 27\n",
      "  17 50 24  4  9 14 28 54]\n",
      " [ 0  6 17  6 31 25 20 48 15 20  6 17 33 27 24 21 32 20 45 17 45 15 13 52\n",
      "   6 17 28 54 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "print(as_matrix(phrase_list[:5], token_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tokens_id.json\", \"w\") as fp:\n",
    "    json.dump({\"token_to_id\":token_to_id, \"id_to_token\": id_to_token, \"tokens\": tokens}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tokens_id.json\") as f:\n",
    "    tokens_id = json.load(f)\n",
    "token_to_id = tokens_id[u'token_to_id']\n",
    "id_to_token = tokens_id[\"id_to_token\"]\n",
    "tokens = tokens_id[\"tokens\"]\n",
    "tokens = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сеть, которая будет получать на вход вопрос и генерировать посимвольно ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu5,floatX=float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 5: GeForce GTX 1080 (CNMeM is enabled with initial size: 22.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=device=gpu5,floatX=float32\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from lasagne.updates import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence','int32')\n",
    "target_phonemes = T.matrix('target phonemes','int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1))\n",
    "l_emb = lasagne.layers.EmbeddingLayer(l_in, len(tokens), 40)\n",
    "l_rnn = lasagne.layers.LSTMLayer(l_emb,256,mask_input=l_mask)\n",
    "l_rnn = lasagne.layers.LSTMLayer(l_rnn,256,only_return_final=True,mask_input=l_mask)\n",
    "\n",
    "##DECODER\n",
    "transc_in = lasagne.layers.InputLayer(shape=(None, None),input_var=target_phonemes)\n",
    "transc_mask = lasagne.layers.InputLayer(shape=(None, None),input_var=T.neq(target_phonemes,-1))\n",
    "transc_emb = lasagne.layers.EmbeddingLayer(transc_in, len(tokens), 50)\n",
    "transc_rnn = lasagne.layers.LSTMLayer(transc_emb,256,hid_init=l_rnn,mask_input=transc_mask)\n",
    "transc_rnn = lasagne.layers.LSTMLayer(transc_rnn,256,hid_init=l_rnn,mask_input=transc_mask)\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "transc_rnn_flat = lasagne.layers.reshape(transc_rnn, (-1,transc_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(transc_rnn_flat,len(tokens),nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, b]\n"
     ]
    }
   ],
   "source": [
    "weights = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "network_output = network_output.reshape([target_phonemes.shape[0], target_phonemes.shape[1], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_flat = network_output[:,:-1,:].reshape([-1, len(tokens)])\n",
    "targets = target_phonemes[:,1:].ravel()\n",
    "\n",
    "mask = T.nonzero(T.neq(targets, -1))\n",
    "\n",
    "loss = categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "updates = adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компилируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#training\n",
    "train = theano.function([input_sequence, target_phonemes], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_phonemes], loss, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим генератор ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "#reshape back into original shape\n",
    "network_output = network_output.reshape((target_phonemes.shape[0],target_phonemes.shape[1],len(tokens)))\n",
    "#predictions for next tokens (after sequence end)\n",
    "last_word_probas = network_output[:,-1]\n",
    "probs = theano.function([input_sequence,target_phonemes],last_word_probas,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(question,answer_prefix = (\"START\",),t=1,sample=True, max_len=500):\n",
    "    \n",
    "    answer = list(answer_prefix)\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        next_let_probs = probs(as_matrix([question],token_to_id),as_matrix([answer],token_to_id) ).ravel()\n",
    "        next_let_probs = next_let_probs**t / np.sum(next_let_probs**t)\n",
    "\n",
    "        if sample:\n",
    "            next_letter = np.random.choice(tokens,p=next_let_probs) \n",
    "        else:\n",
    "            next_letter = tokens[np.argmax(next_let_probs)]\n",
    "        \n",
    "        answer.append(next_letter)\n",
    "\n",
    "        if next_letter==\"END\":\n",
    "            break\n",
    "    return \"\".join(answer[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лаSTARTнsSTARTфд?iхя г льсжduъньйчцфiьpс\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(\"Привет\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим предобученые веса, если есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load(\"Networks_weights.npz\", encoding=\"bytes\") as weights_file:\n",
    "    lasagne.layers.set_all_param_values(l_out, weights_file[\"arr_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(source, batchsize, shuffle=True):\n",
    "    source = np.array(source)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(source)-1)\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(source) -1 - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = np.array(slice(start_idx, start_idx + batchsize))\n",
    "        yield as_matrix(source[excerpt], token_to_id), as_matrix(source[excerpt+1],token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "telegram_token = config.telegram_token\n",
    "telegram_chat_id = config.telegram_chat_id# id чата, в который будут отсылаться результаты в процессе обучения, так как она учится долго\n",
    "dropbox_token = config.dropbox_token# Свежие веса загружаются на в облако, чтобы не потерять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from telebot import TeleBot\n",
    "bot = TeleBot(telegram_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dropbox\n",
    "\n",
    "client = dropbox.client.DropboxClient(dropbox_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_message = \"Epoch {epoch}:\\nTime {time_:.2f} hours\\nAverage loss: {avg_los:.5f}\\nExample sample=True: {sample_0}\"\n",
    "train_error_messages = \"Error on epoch when we train model: {}\\n\\t{}\\nBatch:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-92c6b67d64d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelegram_chat_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_error_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py35/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py35/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "num_epoch = 100\n",
    "batch_per_epoch = 1000\n",
    "batch_size = 100\n",
    "bot.send_message(145718567, \"Learning is begin!\")\n",
    "for epoch in range(num_epoch):\n",
    "    try:\n",
    "        \n",
    "        st = time()\n",
    "        avg_cost = 0\n",
    "        for batch_num, batch in enumerate(iterate_minibatches(phrase_list, batch_size)):\n",
    "            try:\n",
    "                avg_cost += train(batch[0], batch[1])\n",
    "            except Exception as er:\n",
    "                bot.send_message(telegram_chat_id, train_error_messages.format(epoch+1, er, batch))\n",
    "                break\n",
    "                \n",
    "            if batch_num+1 == batch_per_epoch:\n",
    "                break\n",
    "                \n",
    "        weights_file = \"Networks_weights.npz\"\n",
    "        np.savez(weights_file, layers.get_all_param_values(l_out))\n",
    "#         try:\n",
    "#             with open(weights_file, 'rb') as weights_file_dr:\n",
    "#                 response = client.put_file('/' + weights_file, weights_file_dr)\n",
    "\n",
    "#         except Exception as er:\n",
    "#             bot.send_message(telegram_chat_id, \"Error of download to Dropbox: {}\".format(e))\n",
    "\n",
    "        bot.send_message(145718567, train_message.format(time_=(time()-st)/3600,\n",
    "                                                         epoch=epoch+1,\n",
    "                                                         avg_los=avg_cost/batch_per_epoch,\n",
    "                                                         sample_0=generate_answer(\"Здравствуй\")))\n",
    "    \n",
    "    except Exception as er:\n",
    "        bot.send_message(telegram_chat_id, \"Error on epoch: {}\\n\\t{}\".format(epoch+1, er))\n",
    "        break\n",
    "bot.send_message(telegram_chat_id, \"Learning is end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'беспеться; нешенька упержи маменьку не сославне тятеновену.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"Как тебя зовут?\", t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
